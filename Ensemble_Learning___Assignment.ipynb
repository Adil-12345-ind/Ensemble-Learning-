{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-014"
      ],
      "metadata": {
        "id": "BZNInPpThww-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning | Assignment"
      ],
      "metadata": {
        "id": "l1u8O616iWFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        " - Ensemble Learning in machine learning is a technique where multiple models (called \"weak learners\" or base models) are combined to produce a single, more accurate and robust predictive model. Instead of relying on one model, we aggregate the outputs of several models to reduce errors and improve performance.\n",
        "\n",
        "‚úÖ Key Idea Behind Ensemble Learning\n",
        "\n",
        "The core principle is:\n",
        "\n",
        "‚ÄúA group of weak models, when combined properly, can perform better than any individual strong model.‚Äù\n",
        "\n",
        "This works because different models make different errors, and combining them can cancel out those individual mistakes.\n",
        "\n",
        "‚úÖ Why Does It Work?\n",
        "\n",
        "Variance Reduction: By averaging multiple predictions (as in Bagging), we reduce overfitting.\n",
        "\n",
        "Bias Reduction: By combining models in a clever way (as in Boosting), we reduce underfitting.\n",
        "\n",
        "Better Generalization: Different models capture different aspects of the data.\n",
        "\n",
        "‚úÖ Types of Ensemble Methods\n",
        "\n",
        "Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Trains multiple models on random subsets of the data.\n",
        "\n",
        "Example: Random Forest (ensemble of Decision Trees).\n",
        "\n",
        "Goal: Reduce variance.\n",
        "\n",
        "Boosting\n",
        "\n",
        "Models are trained sequentially; each new model focuses on correcting the errors of the previous one.\n",
        "\n",
        "Examples: AdaBoost, Gradient Boosting, XGBoost.\n",
        "\n",
        "Goal: Reduce bias.\n",
        "\n",
        "Stacking\n",
        "\n",
        "Combines multiple different types of models using a meta-model that learns how to best combine their predictions.\n",
        "\n",
        "Example: Combine Logistic Regression, Decision Trees, and SVM using another model like Linear Regression.\n",
        "\n",
        "‚úÖ Real-World Analogy\n",
        "\n",
        "Think of an exam: instead of asking one expert for the answer, you ask a panel of experts from different fields and combine their opinions. The final decision is often more accurate because errors by one expert are compensated by others."
      ],
      "metadata": {
        "id": "k08zD5WGiajz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Bagging and Boosting?\n",
        " - ‚úÖ 1. Definition\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Builds multiple independent models in parallel on random subsets of the training data (with replacement).\n",
        "\n",
        "Combines predictions by averaging (for regression) or majority voting (for classification).\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Builds models sequentially, where each new model focuses on the mistakes of the previous ones.\n",
        "\n",
        "Combines predictions by weighted voting or weighted sum.\n",
        "\n",
        "‚úÖ 2. Key Idea\n",
        "\n",
        "Bagging: Reduce variance by averaging many models trained on different subsets ‚Üí helps avoid overfitting.\n",
        "\n",
        "Boosting: Reduce bias by making weak models learn from errors ‚Üí makes the model more accurate.\n",
        "\n",
        "‚úÖ 3. Training Approach\n",
        "\n",
        "Bagging:\n",
        "\n",
        "Each model is trained independently on a random bootstrap sample.\n",
        "\n",
        "All models have equal weight in the final prediction.\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Models are trained one after another.\n",
        "\n",
        "Later models give more importance to misclassified points (by adjusting weights).\n",
        "\n",
        "‚úÖ 4. Performance\n",
        "\n",
        "Bagging: Works best with high variance models (like Decision Trees).\n",
        "\n",
        "Boosting: Works well with weak learners that have high bias (like shallow trees).\n",
        "\n",
        "‚úÖ 5. Common Algorithms\n",
        "\n",
        "Bagging: Random Forest\n",
        "\n",
        "Boosting: AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost\n",
        "\n",
        "‚úÖ Comparison Table\n",
        "Feature\tBagging\tBoosting\n",
        "Training\tParallel\tSequential\n",
        "Focus\tReduce variance\tReduce bias\n",
        "Data sampling\tBootstrap samples\tFull dataset (with weights)\n",
        "Model weight\tEqual\tBased on performance\n",
        "Overfitting\tLess prone\tCan overfit if too many rounds\n",
        "\n",
        "Analogy:\n",
        "\n",
        "Bagging = Democracy (everyone votes equally)\n",
        "\n",
        "Boosting = Mentorship (each teacher corrects mistakes of the previous one)"
      ],
      "metadata": {
        "id": "vvEmjKIJitwd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        " - ‚úÖ What is Bootstrap Sampling?\n",
        "\n",
        "Bootstrap sampling is a statistical resampling technique where we create new datasets (called bootstrap samples) by randomly selecting data points from the original dataset with replacement.\n",
        "\n",
        "With replacement means: after picking a data point, we put it back before picking the next one.\n",
        "\n",
        "This allows the same data point to appear multiple times in the sample, while some points may not appear at all.\n",
        "\n",
        "Each bootstrap sample is usually the same size as the original dataset, but due to replacement, about 63% of the original data points appear at least once, and the rest are left out.\n",
        "\n",
        "‚úÖ Role in Bagging (e.g., Random Forest)\n",
        "\n",
        "Bagging = Bootstrap Aggregating, and bootstrap sampling plays a critical role:\n",
        "\n",
        "Creates Diversity:\n",
        "\n",
        "Each model (e.g., a Decision Tree) in the ensemble is trained on a different bootstrap sample.\n",
        "\n",
        "This makes the models less correlated, because each tree sees a slightly different version of the data.\n",
        "\n",
        "Reduces Variance:\n",
        "\n",
        "Individual Decision Trees are high-variance models (they can overfit).\n",
        "\n",
        "By averaging predictions from multiple diverse trees, Bagging reduces variance, making predictions more stable and accurate.\n",
        "\n",
        "Out-of-Bag (OOB) Estimation:\n",
        "\n",
        "The data points not included in a bootstrap sample (about 37%) can be used as a validation set to estimate error without needing a separate test set.\n",
        "\n",
        "‚úÖ Example:\n",
        "\n",
        "Suppose you have 100 training samples:\n",
        "\n",
        "Tree 1 might get [1, 3, 3, 5, 6, 6, 8 ‚Ä¶]\n",
        "\n",
        "Tree 2 might get [2, 4, 4, 7, 9, 10, 10 ‚Ä¶]\n",
        "\n",
        "Each tree sees a different random mix of samples, so they learn different patterns.\n",
        "\n",
        "‚úÖ In Random Forest:\n",
        "\n",
        "Each tree is trained on a bootstrap sample.\n",
        "\n",
        "Additionally, feature randomness is introduced (only a subset of features is considered at each split).\n",
        "\n",
        "Together, bootstrap sampling + feature randomness make Random Forest highly robust.\n",
        "\n",
        "Analogy:\n",
        "Imagine training 10 different chefs to make the same dish, but each chef gets slightly different ingredients. When you taste all dishes and average their flavor, the overall taste is more balanced than relying on one chef‚Äôs version."
      ],
      "metadata": {
        "id": "iaRvMTL-jUNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        " - ‚úÖ What are Out-of-Bag (OOB) Samples?\n",
        "\n",
        "When creating bootstrap samples for each tree in Bagging, we sample with replacement from the original dataset.\n",
        "\n",
        "Because of this, not all data points are selected for a given tree.\n",
        "\n",
        "On average, about 63% of the data appears in each bootstrap sample, leaving around 37% of the data out.\n",
        "\n",
        "These unused data points for that tree are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "‚úÖ Why does this happen?\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "For each data point, the probability of not being selected in one draw =\n",
        "1\n",
        "‚àí\n",
        "1\n",
        "ùëõ\n",
        "1‚àí\n",
        "n\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "After\n",
        "ùëõ\n",
        "n draws (for a sample size of\n",
        "ùëõ\n",
        "n), probability of never being chosen =\n",
        "\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "1\n",
        "ùëõ\n",
        ")\n",
        "ùëõ\n",
        "‚âà\n",
        "ùëí\n",
        "‚àí\n",
        "1\n",
        "‚âà\n",
        "0.368\n",
        "(1‚àí\n",
        "n\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "n\n",
        "‚âàe\n",
        "‚àí1\n",
        "‚âà0.368\n",
        "\n",
        "So, ~36.8% of points are OOB for each tree.\n",
        "\n",
        "‚úÖ How is OOB Score Used?\n",
        "\n",
        "The OOB score is an internal cross-validation estimate of model performance, calculated without using a separate validation set:\n",
        "\n",
        "For each observation:\n",
        "\n",
        "Find all trees where this observation was OOB (not in the bootstrap sample).\n",
        "\n",
        "Predict using those trees only.\n",
        "\n",
        "Aggregate predictions:\n",
        "\n",
        "For classification: Use majority vote from OOB trees.\n",
        "\n",
        "For regression: Use the average prediction from OOB trees.\n",
        "\n",
        "Compare with actual labels:\n",
        "\n",
        "Compute accuracy (for classification) or error (for regression).\n",
        "\n",
        "This gives the OOB score, which is similar to test set performance.\n",
        "\n",
        "‚úÖ Benefits of OOB Score\n",
        "\n",
        "‚úî No need for a separate validation set ‚Üí saves data.\n",
        "‚úî Acts like built-in cross-validation for Bagging methods.\n",
        "‚úî Gives an unbiased performance estimate.\n",
        "\n",
        "‚úÖ Example (Random Forest)\n",
        "\n",
        "Dataset = 1000 samples, Random Forest with 100 trees.\n",
        "\n",
        "For a given sample:\n",
        "\n",
        "Appears in ~63 trees, is OOB in ~37 trees.\n",
        "\n",
        "Predict using those 37 trees ‚Üí compare with actual label.\n",
        "\n",
        "Repeat for all samples ‚Üí compute OOB accuracy (e.g., 92%).\n",
        "\n",
        "Analogy:\n",
        "Imagine a classroom where every student is graded by teachers who haven‚Äôt seen their homework before. That‚Äôs what OOB evaluation does‚Äîit ensures an unbiased check."
      ],
      "metadata": {
        "id": "pCbXq-5kjr4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        " - ‚úÖ Feature Importance in a Single Decision Tree\n",
        "\n",
        "Each split in a Decision Tree is made based on a criterion such as Gini Impurity or Information Gain (Entropy).\n",
        "\n",
        "Feature importance for a feature =\n",
        "Total reduction in impurity it brings across all nodes where it is used.\n",
        "\n",
        "Formula:\n",
        "\n",
        "Importance¬†of¬†feature\n",
        "ùëó\n",
        "=\n",
        "‚àë\n",
        "nodes¬†where¬†feature¬†j¬†is¬†used\n",
        "Weighted¬†impurity¬†decrease\n",
        "Total¬†impurity¬†decrease¬†over¬†all¬†nodes\n",
        "Importance¬†of¬†feature¬†j=\n",
        "nodes¬†where¬†feature¬†j¬†is¬†used\n",
        "‚àë\n",
        "\t‚Äã\n",
        "\n",
        "Total¬†impurity¬†decrease¬†over¬†all¬†nodes\n",
        "Weighted¬†impurity¬†decrease\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Simple and intuitive because it‚Äôs based on that tree‚Äôs structure.\n",
        "\n",
        "Sensitive to overfitting (a deep tree may overestimate the importance of some features).\n",
        "\n",
        "High variance ‚Üí if you grow another tree with slightly different data, the importance ranking can change a lot.\n",
        "\n",
        "‚úÖ Feature Importance in Random Forest\n",
        "\n",
        "A Random Forest consists of many Decision Trees trained on different bootstrap samples with feature randomness.\n",
        "\n",
        "Feature importance in a Random Forest is:\n",
        "\n",
        "Average¬†importance¬†of¬†feature\n",
        "ùëó\n",
        "¬†across¬†all¬†trees.\n",
        "Average¬†importance¬†of¬†feature¬†j¬†across¬†all¬†trees.\n",
        "\n",
        "Each tree contributes its feature importances, and the forest averages them.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "More stable and robust than a single tree.\n",
        "\n",
        "Less biased toward a single feature because of multiple trees.\n",
        "\n",
        "Still biased toward features with many categories or high cardinality, but less so than a single tree.\n",
        "\n",
        "‚úÖ Key Differences\n",
        "Aspect\tSingle Decision Tree\tRandom Forest\n",
        "Calculation\tBased on one tree‚Äôs impurity decrease\tAverage of all trees‚Äô impurity decrease\n",
        "Stability\tHigh variance (can change a lot)\tLow variance (more stable)\n",
        "Robustness\tProne to overfitting\tReduced overfitting\n",
        "Bias\tMore bias toward dominant features\tLess bias due to averaging\n",
        "‚úÖ Example\n",
        "\n",
        "Suppose features = [Age, Income, Education].\n",
        "\n",
        "A single tree might say:\n",
        "\n",
        "Age = 60% importance, Income = 30%, Education = 10%.\n",
        "\n",
        "A Random Forest (100 trees) might say:\n",
        "\n",
        "Age = 40%, Income = 35%, Education = 25% (more balanced).\n",
        "\n",
        "‚úÖ Advanced Note\n",
        "\n",
        "Random Forests also support Permutation Feature Importance, where the importance of a feature is measured by how much the prediction error increases when the feature‚Äôs values are randomly shuffled. This gives a model-agnostic, less biased estimate."
      ],
      "metadata": {
        "id": "MRlQL6jCj9yv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "‚óè Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "‚óè Train a Random Forest Classifier\n",
        "‚óè Print the top 5 most important features based on feature importance scores.\n",
        " - Here‚Äôs a clean Python program that does exactly that:"
      ],
      "metadata": {
        "id": "AXzRSVV3kZJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ‚úÖ Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# ‚úÖ Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# ‚úÖ Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# ‚úÖ Create a DataFrame for feature names and their importance\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# ‚úÖ Sort by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# ‚úÖ Print top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))\n"
      ],
      "metadata": {
        "id": "dujQ776Pkj6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What this program does\n",
        "\n",
        "‚úî Loads the Breast Cancer dataset.\n",
        "‚úî Trains a Random Forest Classifier with 100 trees.\n",
        "‚úî Calculates feature importance scores.\n",
        "‚úî Sorts them in descending order.\n",
        "‚úî Prints the top 5 features.\n",
        "\n",
        "Example Output (Approx):"
      ],
      "metadata": {
        "id": "Jhk7pGnkkpPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Top 5 Most Important Features:\n",
        "                 Feature  Importance\n",
        "20        worst concave points   0.142\n",
        "27             worst perimeter   0.103\n",
        "23                worst radius   0.091\n",
        "7             mean concave points 0.084\n",
        "22             worst compactness  0.068\n"
      ],
      "metadata": {
        "id": "fjY0KOtukqdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "‚óè Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "‚óè Evaluate its accuracy and compare with a single Decision Tree.\n",
        " - Here‚Äôs a complete Python program that compares a Bagging Classifier (with Decision Trees) and a single Decision Tree on the Iris dataset:\n",
        "\n",
        "‚úÖ Python Code:"
      ],
      "metadata": {
        "id": "OkNXYt0QkuV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ‚úÖ Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# ‚úÖ Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ‚úÖ Single Decision Tree\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "dtree.fit(X_train, y_train)\n",
        "y_pred_tree = dtree.predict(X_test)\n",
        "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
        "\n",
        "# ‚úÖ Bagging Classifier with Decision Trees\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,       # Number of trees\n",
        "    max_samples=1.0,       # Use full bootstrap samples\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# ‚úÖ Print accuracies\n",
        "print(\"Accuracy of Single Decision Tree:\", accuracy_tree)\n",
        "print(\"Accuracy of Bagging Classifier:\", accuracy_bagging)\n"
      ],
      "metadata": {
        "id": "WLizQYKfldyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What this program does\n",
        "\n",
        "‚úî Loads the Iris dataset.\n",
        "‚úî Splits it into training (70%) and testing (30%) sets.\n",
        "‚úî Trains:\n",
        "\n",
        "A single Decision Tree.\n",
        "\n",
        "A Bagging Classifier with 50 Decision Trees as base learners.\n",
        "‚úî Evaluates accuracy on the test set for both models.\n",
        "‚úî Prints and compares the results.\n",
        "\n",
        "‚úÖ Expected Output (Approx)"
      ],
      "metadata": {
        "id": "z8rBKziZlgNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy of Single Decision Tree: 0.9555\n",
        "Accuracy of Bagging Classifier: 0.9777\n"
      ],
      "metadata": {
        "id": "DBbUHaNxloOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bagging Classifier usually performs slightly better and more stable because it reduces variance compared to a single tree."
      ],
      "metadata": {
        "id": "LllFsePBlo8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "‚óè Train a Random Forest Classifier\n",
        "‚óè Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "‚óè Print the best parameters and final accuracy\n",
        " - Here‚Äôs a complete Python program that trains a Random Forest Classifier, tunes max_depth and n_estimators using GridSearchCV, and prints the best parameters and accuracy:\n",
        "\n",
        "‚úÖ Python Code:"
      ],
      "metadata": {
        "id": "atCSFYH3ls_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# ‚úÖ Load dataset (Iris for example)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# ‚úÖ Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ‚úÖ Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# ‚úÖ Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],    # Number of trees\n",
        "    'max_depth': [None, 3, 5, 7]       # Depth of trees\n",
        "}\n",
        "\n",
        "# ‚úÖ GridSearchCV setup\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                 # 5-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# ‚úÖ Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# ‚úÖ Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# ‚úÖ Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Final Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "3BKeRd_7mupT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What this does\n",
        "\n",
        "‚úî Loads Iris dataset.\n",
        "‚úî Splits into train and test sets (70-30 split).\n",
        "‚úî Sets up a parameter grid for:\n",
        "\n",
        "n_estimators (number of trees)\n",
        "\n",
        "max_depth (tree depth)\n",
        "‚úî Uses GridSearchCV for hyperparameter tuning with 5-fold CV.\n",
        "‚úî Prints the best parameters and final accuracy on the test set.\n",
        "\n",
        "‚úÖ Expected Output (Approx)"
      ],
      "metadata": {
        "id": "4KtI7K7-n93U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Best Parameters: {'max_depth': None, 'n_estimators': 150}\n",
        "Final Test Accuracy: 0.9777\n"
      ],
      "metadata": {
        "id": "vOTJjh_YoAeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Values may vary slightly depending on random state and dataset split.)"
      ],
      "metadata": {
        "id": "bfwVv2htoCgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "‚óè Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "‚óè Compare their Mean Squared Errors (MSE)\n",
        " - Here‚Äôs a complete Python program to train Bagging Regressor and Random Forest Regressor on the California Housing dataset, then compare their Mean Squared Error (MSE):\n",
        "\n",
        "‚úÖ Python Code:"
      ],
      "metadata": {
        "id": "cBT1PkyVoImO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# ‚úÖ Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# ‚úÖ Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ‚úÖ Bagging Regressor with Decision Trees\n",
        "bagging_reg = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_reg.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "\n",
        "# ‚úÖ Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# ‚úÖ Print results\n",
        "print(f\"Mean Squared Error (Bagging Regressor): {mse_bagging:.4f}\")\n",
        "print(f\"Mean Squared Error (Random Forest Regressor): {mse_rf:.4f}\")\n"
      ],
      "metadata": {
        "id": "KuIqavpzotwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What this program does\n",
        "\n",
        "‚úî Loads California Housing dataset.\n",
        "‚úî Splits data into train and test sets (70-30 split).\n",
        "‚úî Trains:\n",
        "\n",
        "Bagging Regressor with Decision Tree base estimators.\n",
        "\n",
        "Random Forest Regressor (internally uses feature randomness + bagging).\n",
        "‚úî Predicts on the test set and calculates MSE for both models.\n",
        "‚úî Prints and compares the results.\n",
        "\n",
        "‚úÖ Expected Output (Approx)"
      ],
      "metadata": {
        "id": "xht8IYKno0eW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Mean Squared Error (Bagging Regressor): 0.2945\n",
        "Mean Squared Error (Random Forest Regressor): 0.2321\n"
      ],
      "metadata": {
        "id": "U1bwiHIFo3au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Random Forest usually performs slightly better because it introduces extra randomness in feature selection, making trees less correlated.)"
      ],
      "metadata": {
        "id": "nHzrpGz6o4Dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "‚óè Choose between Bagging or Boosting\n",
        "‚óè Handle overfitting\n",
        "‚óè Select base models\n",
        "‚óè Evaluate performance using cross-validation\n",
        "‚óè Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        " - Here‚Äôs how I would structure a clear, step-by-step solution for this real-world scenario:\n",
        "\n",
        "‚úÖ Step 1: Understand the Problem & Data\n",
        "\n",
        "Goal: Predict loan default (binary classification).\n",
        "\n",
        "Data: Customer demographics + transaction history ‚Üí mixture of categorical and numerical features.\n",
        "\n",
        "Challenges:\n",
        "\n",
        "Class imbalance (typically, far fewer defaults than non-defaults).\n",
        "\n",
        "High stakes ‚Üí wrong prediction can cause financial loss.\n",
        "\n",
        "Non-linear relationships ‚Üí need flexible models.\n",
        "\n",
        "‚úÖ Step 2: Choose Between Bagging and Boosting\n",
        "\n",
        "Bagging (e.g., Random Forest):\n",
        "\n",
        "Reduces variance ‚Üí great for high-variance models like Decision Trees.\n",
        "\n",
        "Works best when base models overfit (deep trees).\n",
        "\n",
        "Boosting (e.g., XGBoost, LightGBM):\n",
        "\n",
        "Reduces bias by sequentially improving weak learners.\n",
        "\n",
        "Handles complex, non-linear patterns well.\n",
        "\n",
        "Often outperforms Bagging on structured/tabular data like financial data.\n",
        "\n",
        "Choice:\n",
        "\n",
        "I would start with Boosting (e.g., XGBoost or LightGBM) because:\n",
        "\n",
        "Data likely has strong non-linear interactions.\n",
        "\n",
        "Boosting handles imbalance better with scale_pos_weight.\n",
        "\n",
        "Proven success in financial risk modeling.\n",
        "\n",
        "But I would benchmark with Bagging (Random Forest) for comparison.\n",
        "\n",
        "‚úÖ Step 3: Handle Overfitting\n",
        "\n",
        "For Bagging:\n",
        "\n",
        "Limit tree depth (max_depth).\n",
        "\n",
        "Increase n_estimators (more trees = better averaging).\n",
        "\n",
        "For Boosting:\n",
        "\n",
        "Use learning rate (Œ∑) to slow down learning (e.g., 0.05).\n",
        "\n",
        "Limit max_depth of trees (e.g., 4‚Äì6).\n",
        "\n",
        "Add regularization (lambda, alpha for XGBoost).\n",
        "\n",
        "Use early stopping with a validation set.\n",
        "\n",
        "Feature selection & engineering:\n",
        "\n",
        "Remove irrelevant/noisy features.\n",
        "\n",
        "Scale numeric features if needed for consistency.\n",
        "\n",
        "‚úÖ Step 4: Select Base Models\n",
        "\n",
        "For Bagging:\n",
        "\n",
        "Base learner = Decision Tree (unpruned) because it has high variance.\n",
        "\n",
        "For Boosting:\n",
        "\n",
        "Base learner = shallow trees (max depth 4‚Äì6) ‚Üí weak learners that Boosting can improve.\n",
        "\n",
        "Why not Logistic Regression or SVM as base models?\n",
        "\n",
        "They have low variance, so Bagging adds little value.\n",
        "\n",
        "Decision Trees are more flexible and capture non-linear interactions.\n",
        "\n",
        "‚úÖ Step 5: Evaluate Performance Using Cross-Validation\n",
        "\n",
        "Use Stratified k-Fold CV (e.g., k=5 or 10) to maintain class balance.\n",
        "\n",
        "Metrics:\n",
        "\n",
        "AUC-ROC (to measure ranking ability for imbalanced data).\n",
        "\n",
        "Precision-Recall AUC (because defaults are rare).\n",
        "\n",
        "Also track F1-score for threshold tuning.\n",
        "\n",
        "Apply GridSearchCV or RandomizedSearchCV for hyperparameter tuning.\n",
        "\n",
        "Use OOB score for Bagging models as an internal validation metric.\n",
        "\n",
        "‚úÖ Step 6: Justify How Ensemble Learning Improves Decision-Making\n",
        "\n",
        "Why better than a single model?\n",
        "\n",
        "Single Decision Tree ‚Üí high variance, prone to overfitting.\n",
        "\n",
        "Logistic Regression ‚Üí linear, underfits complex patterns.\n",
        "\n",
        "Ensemble (Bagging/Boosting) Benefits:\n",
        "\n",
        "Combines multiple models ‚Üí reduces variance (Bagging) or bias (Boosting).\n",
        "\n",
        "Handles noisy, complex data with better generalization.\n",
        "\n",
        "Boosting prioritizes hard-to-predict defaults ‚Üí improves recall, which is critical in risk prediction.\n",
        "\n",
        "Business Value:\n",
        "\n",
        "More accurate risk assessment ‚Üí fewer bad loans ‚Üí reduced financial loss.\n",
        "\n",
        "Explainability: Random Forest and XGBoost provide feature importance, helping justify decisions for regulators.\n",
        "\n",
        "Confidence in predictions improves strategic decision-making for credit approval.\n",
        "\n",
        "‚úÖ Summary Table\n",
        "Step\tTechnique Used\tPurpose\n",
        "Choose method\tBoosting (XGBoost)\tHandle bias, complex patterns\n",
        "Overfitting\tDepth limit, LR, regularization\tImprove generalization\n",
        "Base model\tDecision Tree\tFlexible, non-linear\n",
        "Evaluation\tStratified k-Fold CV\tReliable performance measure\n",
        "Justification\tImproved AUC, recall\tLower financial risk."
      ],
      "metadata": {
        "id": "T7ZSgqnLo_wI"
      }
    }
  ]
}